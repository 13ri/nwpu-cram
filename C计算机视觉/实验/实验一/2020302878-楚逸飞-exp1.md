# **计算机视觉实验一**

## **实验目的**

① 加强对基于Mindspore的神经网络模型构建流程的理解。

②  掌握如何用Mindspore实现卷积神经网络的构建。

③ 学会利用checkpoint函数保存模型参数。

④ 掌握如何利用模型预测单张图像的分类结果。

## **实验内容**

#### 任务一：按照华为平台实验手册进行操作

要求：熟悉实验环境，掌握卷积神经网络模型程序流程
具体：记录并观察实验结果，如损失随迭代轮数变化等

#### 任务二：LeNet-5模型对比

要求：调节实验参数，进行实验对比及分析
     实验参数包括：
     1.训练批次大小，迭代轮数，学习速率，最优化方法等

#### 任务三：卷积神经网络模型设计

要求：改变网络结构，进行实验对比及分析
具体：
     1.卷积层数，卷积核尺寸及步长，激活函数，池化方法，全连接层层数，各层节点数目
     2.添加和不添加BN层

## **实验过程**

#### 任务一：按照华为平台实验手册进行操作

##### 1.创建华为云Notebook

​	1.1进入Modelarts，选择Notebook

![image-20221203101650935](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203101650935.png)

​	1.2创建Notebook

![image-20221203101617818](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203101617818.png)

![image-20221203101733985](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203101733985.png)

​	1.3启动Notebook

![image-20221203101842385](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203101842385.png)

##### 2.导入相关实验模块

```python
import mindspore
# 载入mindspore的默认数据集
import mindspore.dataset as ds
# 常用转化用算子
import mindspore.dataset.transforms.c_transforms as C
# 图像转化用算子
####____####
import mindspore.dataset.vision.c_transforms as CV
from mindspore.common import dtype as mstype
# mindspore的tensor
from mindspore import Tensor


# 各类网络层都在nn里面
import mindspore.nn as nn
# 参数初始化的方式

from mindspore.common.initializer import TruncatedNormal
# 设置mindspore运行的环境
from mindspore import context
# 引入训练时候会使用到回调函数，如checkpoint, lossMoniter
from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor
# 引入模型
from mindspore.train import Model
# 引入评估模型的包
from mindspore.nn.metrics import Accuracy

# numpy
import numpy as np
# 画图用
import matplotlib.pyplot as plt

####____####
# 下载数据相关的包
import os
import requests 
import zipfile
```



##### 3.数据集展示与数据初始化

​	3.1数据集下载

![image-20221203102210608](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203102210608.png)

在terminal中查看

![image-20221203102253952](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203102253952.png)

​	3.2查看数据集

```python
#创建图像标签列表
category_dict = {0:'airplane',1:'automobile',2:'bird',3:'cat',4:'deer',5:'dog',
                 6:'frog',7:'horse',8:'ship',9:'truck'}

####____####
current_path = os.getcwd()
data_path = os.path.join(current_path, 'data/10-verify-bin')
cifar_ds = ds.Cifar10Dataset(data_path)

# 设置图像大小
plt.figure(figsize=(8,8))
i = 1
# 打印9张子图
for dic in cifar_ds.create_dict_iterator():
    plt.subplot(3,3,i)
    ####____####
    plt.imshow(dic['image'].asnumpy())
    plt.xticks([])
    plt.yticks([])
    plt.axis('off')
    plt.title(category_dict[dic['label'].asnumpy().sum()])
    i +=1
    if i > 9 :
        break

plt.show()
```

![image-20221203102513143](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203102513143.png)

​	3.3定义数据预处理步骤

```python
def get_data(datapath):
    cifar_ds = ds.Cifar10Dataset(datapath)
    return cifar_ds

def process_dataset(cifar_ds,batch_size =32,status="train"):
    '''
    ---- 定义算子 ----
    '''
    # 归一化
    rescale = 1.0 / 255.0
    # 平移
    shift = 0.0

    resize_op = CV.Resize((32, 32))
    rescale_op = CV.Rescale(rescale, shift)
    # 对于RGB三通道分别设定mean和std
    normalize_op = CV.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    if status == "train":
        # 随机裁剪
        random_crop_op = CV.RandomCrop([32, 32], [4, 4, 4, 4])
        # 随机翻转
        random_horizontal_op = CV.RandomHorizontalFlip()
    # 通道变化
    channel_swap_op = CV.HWC2CHW()
    # 类型变化
    typecast_op = C.TypeCast(mstype.int32)

    '''
    ---- 算子运算 ----
    '''
    cifar_ds = cifar_ds.map(input_columns="label", operations=typecast_op)
    if status == "train":
        cifar_ds = cifar_ds.map(input_columns="image", operations=random_crop_op)
        cifar_ds = cifar_ds.map(input_columns="image", operations=random_horizontal_op)
    cifar_ds = cifar_ds.map(input_columns="image", operations=resize_op)
    cifar_ds = cifar_ds.map(input_columns="image", operations=rescale_op)
    cifar_ds = cifar_ds.map(input_columns="image", operations=normalize_op)
    cifar_ds = cifar_ds.map(input_columns="image", operations=channel_swap_op)
    
    # shuffle
    cifar_ds = cifar_ds.shuffle(buffer_size=1000)
    # 切分数据集到batch_size
    cifar_ds = cifar_ds.batch(batch_size, drop_remainder=True)
    
    return cifar_ds
```

​	

3.4生成训练数据集

```python
data_path = os.path.join(current_path, 'data/10-batches-bin')
batch_size=32
status="train"

# 生成训练数据集
cifar_ds = get_data(data_path)
ds_train = process_dataset(cifar_ds,batch_size =batch_size, status=status)
```



##### 4.构建网络模型

​	4.1定义LeNet网络结构，构建网络

```python
"""LeNet."""


def conv(in_channels, out_channels, kernel_size, stride=1, padding=0):
    """weight initial for conv layer"""
    weight = weight_variable()
    return nn.Conv2d(in_channels, out_channels,
                     kernel_size=kernel_size, stride=stride, padding=padding,
                     weight_init=weight, has_bias=False, pad_mode="same")


def fc_with_initialize(input_channels, out_channels):
    """weight initial for fc layer"""
    weight = weight_variable()
    bias = weight_variable()
    return nn.Dense(input_channels, out_channels, weight, bias)


def weight_variable():
    """weight initial"""
    return TruncatedNormal(0.02)


class LeNet5(nn.Cell):
    """
    Lenet network

    Args:
        num_class (int): Num classes. Default: 10.

    Returns:
        Tensor, output tensor
    Examples:
        >>> LeNet(num_class=10)

    """
    def __init__(self, num_class=10, channel=3):
        super(LeNet5, self).__init__()
        self.num_class = num_class
        self.conv1 = conv(channel, 6, 5)
        self.conv2 = conv(6, 16, 5)
        self.fc1 = fc_with_initialize(16 * 8 * 8, 120)
        self.fc2 = fc_with_initialize(120, 84)
        self.fc3 = fc_with_initialize(84, self.num_class)
        self.relu = nn.ReLU()
        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)
        self.flatten = nn.Flatten()

    def construct(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        x = self.max_pool2d(x)
        x = self.conv2(x)
        x = self.relu(x)
        x = self.max_pool2d(x)
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.relu(x)
        x = self.fc3(x)
        return x
# 构建网络
network = LeNet5(10)
```



##### 5.模型训练与测试

​	5.1定义损失函数与优化器

```python
# 返回当前设备
device_target = mindspore.context.get_context('device_target')
# 确定图模型是否下沉到芯片上
dataset_sink_mode = True if device_target in ['Ascend','GPU'] else False
# 设置模型的设备与图的模式
context.set_context(mode=context.GRAPH_MODE, device_target=device_target)
# 使用交叉熵函数作为损失函数
net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction="mean")
# 优化器为Adam
net_opt = nn.Adam(params=network.trainable_params(), learning_rate=0.001)
# 监控每个epoch训练的时间
time_cb = TimeMonitor(data_size=ds_train.get_dataset_size())

```

​	5.2定义保存路径与训练

```python
from mindspore.train.callback import Callback

class EvalCallBack(Callback):
    def __init__(self, model, eval_dataset, eval_per_epoch, epoch_per_eval):
        self.model = model
        self.eval_dataset = eval_dataset
        self.eval_per_epoch = eval_per_epoch
        self.epoch_per_eval = epoch_per_eval

    def epoch_end(self, run_context):
        cb_param = run_context.original_args()
        cur_epoch = cb_param.cur_epoch_num
        if cur_epoch % self.eval_per_epoch == 0:
            acc = self.model.eval(self.eval_dataset, dataset_sink_mode=False)
            self.epoch_per_eval["epoch"].append(cur_epoch)
            self.epoch_per_eval["acc"].append(acc["Accuracy"])
            print(acc)
# 设置CheckpointConfig，callback函数。save_checkpoint_steps=训练总数/batch_size
config_ck = CheckpointConfig(save_checkpoint_steps=1562,
                             keep_checkpoint_max=10)
ckpoint_cb = ModelCheckpoint(prefix="checkpoint_lenet_original", directory='./results',config=config_ck)
# 建立可训练模型
model = Model(network = network, loss_fn=net_loss,optimizer=net_opt, metrics={"Accuracy": Accuracy()})
eval_per_epoch = 1
epoch_per_eval = {"epoch": [], "acc": []}
eval_cb = EvalCallBack(model, ds_train, eval_per_epoch, epoch_per_eval)
print("============== Starting Training ==============")
model.train(20, ds_train,callbacks=[ckpoint_cb, LossMonitor(per_print_times=1),eval_cb],dataset_sink_mode=dataset_sink_mode)
```

训练过程：

![image-20221203105044761](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203105044761.png)	

​	5.3设置测试集参数并测试

```python
data_path = os.path.join(current_path, 'data/10-verify-bin')
batch_size=32
status="test"
# 生成测试数据集
cifar_ds = ds.Cifar10Dataset(data_path)
ds_eval = process_dataset(cifar_ds,batch_size=batch_size,status=status)

res = model.eval(ds_eval, dataset_sink_mode=dataset_sink_mode)
# 评估测试集
print('test results:',res)
```

![image-20221203105240108](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203105240108.png)	

​	5.4图片类别预测与可视化

```python
#创建图像标签列表
category_dict = {0:'airplane',1:'automobile',2:'bird',3:'cat',4:'deer',5:'dog',
                 6:'frog',7:'horse',8:'ship',9:'truck'}

cifar_ds = get_data('./data/10-verify-bin')
df_test = process_dataset(cifar_ds,batch_size=1,status='test')

def normalization(data):
    _range = np.max(data) - np.min(data)
    return (data - np.min(data)) / _range

# 设置图像大小
plt.figure(figsize=(10,10))
i = 1
# 打印9张子图
for dic in df_test:
    # 预测单张图片
    input_img = dic[0]    
    output = model.predict(Tensor(input_img))
    output = nn.Softmax()(output)
    # 反馈可能性最大的类别
    predicted = np.argmax(output.asnumpy(),axis=1)[0]
    
    # 可视化
    plt.subplot(3,3,i)
    # 删除batch维度
    input_image = np.squeeze(input_img.asnumpy(),axis=0).transpose(1,2,0)
    # 重新归一化，方便可视化
    input_image = normalization(input_image)
    plt.imshow(input_image)
    plt.xticks([])
    plt.yticks([])
    plt.axis('off')
    plt.title('True label:%s,\n Predicted:%s'%(category_dict[dic[1].asnumpy().sum()],category_dict[predicted]))
    i +=1
    if i > 9 :
        break

plt.show()
```

预测结果：

![image-20221203105323083](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203105323083.png)

#### 任务二：LeNet-5模型对比

##### 1.调整训练数据和测试数据比例及多少

​	1.1 训练数据

​	1.2 测试数据比例

##### 2.训练批次大小，迭代轮数，学习速率，最优化方法等

​	2.1 训练批次大小

​	2.2 迭代轮数

​	2.3 学习速率

​	2.4 最优化方法



#### 任务三：卷积神经网络模型设计

​	自己设计的网络：

卷积核从5*5变为3\*3；增加两层卷积层，提升模型的非线性映射能力；提升卷积核数量为128；在每一层网络中加入BN层

```python
class LeNet5_improve(nn.Cell):
    """
    Lenet network
    

    Args:
        num_class (int): Num classes. Default: 10.

    Returns:
        Tensor, output tensor
    Examples:
        >>> LeNet(num_class=10)

    """
    def __init__(self, num_class=10, channel=3):
        super(LeNet5_improve, self).__init__()
        self.num_class = num_class
        self.conv1_1 = conv(channel, 12, 3)
        self.bn2_1 = nn.BatchNorm2d(num_features=12)
        self.conv1_2 = conv(12, 24, 3)
        self.bn2_2 = nn.BatchNorm2d(num_features=24)        
        self.conv2_1 = conv(24, 48, 3)
        self.bn2_3 = nn.BatchNorm2d(num_features=48)        
        self.conv2_2 = conv(48, 96, 3)
        self.bn2_4 = nn.BatchNorm2d(num_features=96)
        self.fc1 = fc_with_initialize(96*8*8, 160)
        self.bn1_1 = nn.BatchNorm1d(num_features=160)
        self.fc2 = fc_with_initialize(160, 120)
        self.bn1_2 = nn.BatchNorm1d(num_features=120)
        self.fc3 = fc_with_initialize(120, self.num_class)
        self.relu = nn.ReLU()      
        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)
        self.flatten = nn.Flatten()
        
        

    def construct(self, x):
        x = self.conv1_1(x)
        x = self.bn2_1(x)
        x = self.relu(x)
        x = self.conv1_2(x)
        x = self.bn2_2(x)
        x = self.relu(x)
        x = self.max_pool2d(x)
        x = self.conv2_1(x)
        x = self.bn2_3(x)
        x = self.relu(x)
        x = self.conv2_2(x)
        x = self.bn2_4(x)
        x = self.relu(x)
        x = self.max_pool2d(x)        
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.bn1_1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.bn1_2(x)
        x = self.relu(x)
        x = self.fc3(x)
        return x
```

```python
ata_path = os.path.join(current_path, 'data/10-batches-bin')
batch_size=32
status="train"

# 生成训练数据集
cifar_ds = get_data(data_path)
ds_train = process_dataset(cifar_ds,batch_size =batch_size, status=status)
network = LeNet5_improve(10)
#network = resnet50(10)
# 返回当前设备
device_target = mindspore.context.get_context('device_target')
# 确定图模型是否下沉到芯片上
dataset_sink_mode = True if device_target in ['Ascend','GPU'] else False
# 设置模型的设备与图的模式
context.set_context(mode=context.GRAPH_MODE, device_target=device_target)
# 使用交叉熵函数作为损失函数
net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction="mean")
# 优化器为momentum
#net_opt = nn.Momentum(params=network.trainable_params(), learning_rate=0.01, momentum=0.9)
net_opt = nn.Adam(params=network.trainable_params(), learning_rate=0.001)
# 时间监控，反馈每个epoch的运行时间
time_cb = TimeMonitor(data_size=ds_train.get_dataset_size())
# 设置callback函数。
config_ck = CheckpointConfig(save_checkpoint_steps=1562,
                             keep_checkpoint_max=10)
ckpoint_cb = ModelCheckpoint(prefix="checkpoint_lenet_2_verified",directory='./results', config=config_ck)
# 建立可训练模型
model = Model(network = network, loss_fn=net_loss,optimizer=net_opt, metrics={"Accuracy": Accuracy()})
eval_per_epoch = 1
epoch_per_eval = {"epoch": [], "acc": []}
eval_cb = EvalCallBack(model, ds_train, eval_per_epoch, epoch_per_eval)
print("============== Starting Training ==============")

model.train(30, ds_train,callbacks=[ckpoint_cb, LossMonitor(per_print_times=1),eval_cb],dataset_sink_mode=dataset_sink_mode)

```

训练结果：

![image-20221203112115484](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203112115484.png)

```python
data_path = os.path.join(current_path, 'data/10-verify-bin')
batch_size=32
status="test"
# 生成测试数据集
cifar_ds = ds.Cifar10Dataset(data_path)
ds_eval = process_dataset(cifar_ds,batch_size=batch_size,status=status)

res = model.eval(ds_eval, dataset_sink_mode=dataset_sink_mode)
# 评估测试集
print('test results:',res)


```

![image-20221203112348935](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203112348935.png)

![image-20221203113748392](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203113748392.png)



#### 任务二：LeNet-5模型对比

在任务二中，通过改变训练数据的多少、训练批次、迭代轮数、学习速率等与任务一中的结果进行比对。

下图是任务一中的训练结果：

![image-20221203112115484](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203112115484.png)



1.训练批次大小，迭代轮数，学习速率，最优化方法等

这是任务一中模型的训练结果：

​	1.1 改变训练批次大小

​	将batch_size从 32 改为 64，结果：

![image-20221203215933557](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203215933557.png)

![image-20221203215905590](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203215905590.png)

![image-20221203215925291](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203215925291.png)

将batch_size再改为128，结果：

![image-20221203220532500](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203220532500.png)

![image-20221203220522219](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203220522219.png)

![image-20221203220553001](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203220553001.png)



​	1.2 改变迭代轮数

​	将epoch从20改为30，结果：

![image-20221203221949207](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203221949207.png)

![image-20221203221940999](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203221940999.png)

![image-20221203222004766](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203222004766.png)



​	将epoch从30改为40，结果：

![image-20221203212408183](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203212408183.png)

![image-20221203212434267](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203212434267.png)

​	1.3 改变学习速率

​	learning_rate从0.001改为0.005：

![image-20221203213956099](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203213956099.png)

![image-20221203213940858](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203213940858.png)

![image-20221203214019116](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203214019116.png)

learning_rate从0.005改为0.01：（learning_rate = 0.01的时候loss很大且一直降不下来，accuracy一直非常低，说明此时learning_rate过大，模型无法学到有用的特征，已经没有必要再增加learning_rate了）

![image-20221203214729128](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203214729128.png)

![image-20221203222310707](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203222310707.png)

![image-20221203214744404](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203214744404.png)



​	1.4 改变最优化方法

​	最优化方法设置为momentum结果：

![image-20221203230534409](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203230534409.png)

测试集上结果：

![image-20221203230607195](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203230607195.png)

效果没有阿达姆好，但相差并不大。



#### 任务三：卷积神经网络模型设计

改进的神经网络LeNet5_improve（算是比较成功吧，测试的acc有0.9）

所有的卷积核从5∗5变成3∗3。

增加了两层卷积层，提升模型的非线性映射能力。

提升了卷积核数量，以及linear层的输出width，使模型可以提取更多的特征（12 -> 24 -> 48 -> 96 -> 160 -> 120）。

在每一层网络层中加入 BN（批归一化）层。



```python
class LeNet5_improve(nn.Cell):
    """
    Lenet network
    

    Args:
        num_class (int): Num classes. Default: 10.

    Returns:
        Tensor, output tensor
    Examples:
        >>> LeNet(num_class=10)

    """
    def __init__(self, num_class=10, channel=3):
        super(LeNet5_improve, self).__init__()
        self.num_class = num_class
        self.conv1_1 = conv(channel, 12, 3)
        self.bn2_1 = nn.BatchNorm2d(num_features=12)
        self.conv1_2 = conv(12, 24, 3)
        self.bn2_2 = nn.BatchNorm2d(num_features=24)        
        self.conv2_1 = conv(24, 48, 3)
        self.bn2_3 = nn.BatchNorm2d(num_features=48)        
        self.conv2_2 = conv(48, 96, 3)
        self.bn2_4 = nn.BatchNorm2d(num_features=96)
        self.fc1 = fc_with_initialize(96*8*8, 160)
        self.bn1_1 = nn.BatchNorm1d(num_features=160)
        self.fc2 = fc_with_initialize(160, 120)
        self.bn1_2 = nn.BatchNorm1d(num_features=120)
        self.fc3 = fc_with_initialize(120, self.num_class)
        self.relu = nn.ReLU()      
        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)
        self.flatten = nn.Flatten()
        
        

    def construct(self, x):
        x = self.conv1_1(x)
        x = self.bn2_1(x)
        x = self.relu(x)
        x = self.conv1_2(x)
        x = self.bn2_2(x)
        x = self.relu(x)
        x = self.max_pool2d(x)
        x = self.conv2_1(x)
        x = self.bn2_3(x)
        x = self.relu(x)
        x = self.conv2_2(x)
        x = self.bn2_4(x)
        x = self.relu(x)
        x = self.max_pool2d(x)        
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.bn1_1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.bn1_2(x)
        x = self.relu(x)
        x = self.fc3(x)
        return x
```

```python
ata_path = os.path.join(current_path, 'data/10-batches-bin')
batch_size=32
status="train"

# train_loss = []
# train_accuracy = []

# 生成训练数据集
cifar_ds = get_data(data_path)
ds_train = process_dataset(cifar_ds,batch_size =batch_size, status=status)
network = LeNet5_improve(10)
#network = resnet50(10)
# 返回当前设备
device_target = mindspore.context.get_context('device_target')
# 确定图模型是否下沉到芯片上
dataset_sink_mode = True if device_target in ['Ascend','GPU'] else False
# 设置模型的设备与图的模式
context.set_context(mode=context.GRAPH_MODE, device_target=device_target)
# 使用交叉熵函数作为损失函数
net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction="mean")
# 优化器为momentum
#net_opt = nn.Momentum(params=network.trainable_params(), learning_rate=0.01, momentum=0.9)
net_opt = nn.Adam(params=network.trainable_params(), learning_rate=0.001)
# 时间监控，反馈每个epoch的运行时间
time_cb = TimeMonitor(data_size=ds_train.get_dataset_size())
# 设置callback函数。
config_ck = CheckpointConfig(save_checkpoint_steps=1562,
                             keep_checkpoint_max=10)
ckpoint_cb = ModelCheckpoint(prefix="checkpoint_lenet_2_verified",directory='./results', config=config_ck)
# 建立可训练模型
model = Model(network = network, loss_fn=net_loss,optimizer=net_opt, metrics={"Accuracy": Accuracy()})
eval_per_epoch = 1
epoch_per_eval = {"epoch": [], "acc": []}
eval_cb = EvalCallBack(model, ds_train, eval_per_epoch, epoch_per_eval)
print("============== Starting Training ==============")

model.train(30, ds_train,callbacks=[ckpoint_cb, LossMonitor(per_print_times=1),eval_cb],dataset_sink_mode=dataset_sink_mode)

```

训练结果（损失值和精确率的变化）：

![image-20221203201909929](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203201909929.png)

![image-20221203223509335](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203223509335.png)

测试预测精度：

![image-20221203201927218](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203201927218.png)

再拿九张测试一下：七张的结果正确（对比之前4张正确，虽如此小的数据量不严谨但已经说明测试精度有明显提升了）

![image-20221203224301637](C:\Users\YiFei-Chu\Desktop\作业\大三上作业\计算机视觉\实验\实验一\assets\image-20221203224301637.png)

## **实验结果及分析**

结果见上述过程中的展示。

可以得出以下几点结论：

①一定范围内，epoch越大，最终的acc越高，loss越低（这都是平均而言）。实际上当epoch超过一定范围时（一般这个范围还挺大的），继续迭代难以提升模型的效果。并且，epoch是基于train_set的，在train_test上效果好也并不代表在test_set上效果也好，尤其是二者相差较大的时候。为了提升模型的泛化能力，防止over fitting，epoch也不必很大。可以加入dropout来防止overfitting。

②对于learning_rate，并不是越大越好。learining_rate就代表了模型的学习速度，这并不意味着总体上模型能学到更多“好的”东西，learning_rate较大时，模型学习错误信息/特征的能力也更强，所以总体而言，难以断定learning_rate是多大，可以在较小epoch范围内，对不同的learning_rate进行测试试验，绘制acc/loss~learning_rate大致曲线来确定learning_rate的值。当数据集质量很高的时候，learning_rate可能较大比较好，当数据集质量不太高的时候,learning_rate不能设太大，否则模型将在错误的道路上越走越远，变成一无是处的模型。learning_rate=0.001似乎就是最常见的设置方法。（好多模型都是设置的这个值，或者不知道该设置多少的时候也会用0.001先试一下）

③batch_size越大效果似乎会越好，但是效果的提升并不明显。

④对于模型效果的提升，除了epoch、learning_rate等网络以外参数的选择，网络本身的参数更加重要（决定性的）。nn自身的参数，层数的搭建中，ksize越小，其能关注的细节越多，但相应的计算量也会更大。多增加一些conv层和linear层（全连接层，fully-connected层，fc层），可以增加模型的拟合能力。其实就是增加了nn的深度，向深度学习靠近。牺牲性能，提升精度。其实这里网络太深的时候，可解释性并不好，难以解释其特征是如何提取的。网络比较浅的时候，可解释性会好一些。

⑤对于不同的优化器。实际效果与tarin_set中数据的分布有很大关系，也与optimizer的特点有关。

